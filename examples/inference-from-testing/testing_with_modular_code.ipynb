{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebcb20-e9e0-4e7a-96a7-8d5a3bdf6ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "from matplotlib.cm import get_cmap\n",
    "from pickle import dump, load\n",
    "import scipy as sp\n",
    "from scipy.integrate import solve_ivp\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import minimize\n",
    "import scipy as sp\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from numpy.linalg import eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df21d92-b7b1-46f1-a7d5-adccde85073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'inference-from-testing' in os.getcwd():\n",
    "    os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15213e65-0a64-4a17-8edf-0f71dd631404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from numpy import arange, array, atleast_2d, log\n",
    "from os import mkdir\n",
    "from os.path import isdir, isfile\n",
    "from pickle import load, dump\n",
    "from pandas import read_csv\n",
    "from scipy.integrate import solve_ivp\n",
    "from model.preprocessing import ( estimate_beta_ext, estimate_growth_rate,\n",
    "        SEIRInput, HouseholdPopulation, make_initial_condition_by_eigenvector)\n",
    "from model.specs import SINGLE_AGE_SEIR_SPEC_FOR_FITTING, SINGLE_AGE_UK_SPEC\n",
    "from model.common import SEIRRateEquations\n",
    "from model.imports import FixedImportModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4369947-febe-410e-bb5b-466e9e0fdde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random seed for reproducibility\n",
    "\n",
    "np.random.seed(637)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb3bb7-6613-4a5f-80ba-770816a13be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLING_TIME = 3\n",
    "growth_rate = log(2) / DOUBLING_TIME\n",
    "\n",
    "comp_dist = read_csv(\n",
    "    'inputs/england_hh_size_dist.csv',\n",
    "    header=0).to_numpy().squeeze()\n",
    "comp_dist = comp_dist[:3]\n",
    "comp_dist[:2] *= 0\n",
    "comp_dist = comp_dist/sum(comp_dist)\n",
    "max_hh_size = len(comp_dist)\n",
    "composition_list = np.atleast_2d(arange(1, max_hh_size+1)).T\n",
    "\n",
    "#comp_dist = array([1.])\n",
    "#composition_list = array([[1]])\n",
    "\n",
    "if isdir('outputs') is False:\n",
    "    mkdir('outputs')\n",
    "if isdir('outputs/inference-from-testing') is False:\n",
    "    mkdir('outputs/inference-from-testing')\n",
    "\n",
    "SPEC = {**SINGLE_AGE_SEIR_SPEC_FOR_FITTING, **SINGLE_AGE_UK_SPEC}\n",
    "base_sitp = SPEC[\"SITP\"]\n",
    "SPEC[\"SITP\"] = 1 - (1-base_sitp)**3\n",
    "model_input_to_fit = SEIRInput(SPEC, composition_list, comp_dist)\n",
    "household_population_to_fit = HouseholdPopulation(\n",
    "    composition_list, comp_dist, model_input_to_fit)\n",
    "beta_ext = 0.\n",
    "model_input = deepcopy(model_input_to_fit)\n",
    "model_input.k_ext *= beta_ext\n",
    "\n",
    "true_density_expo = model_input.density_expo\n",
    "\n",
    "\n",
    "# With the parameters chosen, we calculate Q_int:\n",
    "household_population = HouseholdPopulation(\n",
    "    composition_list, comp_dist, model_input)\n",
    "\n",
    "true_lam = 3.\n",
    "pop_prev = 1e-2\n",
    "rhs = SEIRRateEquations(model_input, household_population, FixedImportModel(4,1, np.array([true_lam * pop_prev])))\n",
    "\n",
    "H0 = np.zeros((household_population.total_size),)\n",
    "all_sus = np.where(np.sum(rhs.states_exp_only + rhs.states_inf_only + rhs.states_rec_only, 1)<1e-1)[0]\n",
    "one_inf = np.where((np.abs(np.sum(rhs.states_inf_only, 1) - 1)<1e-1) & (np.sum(rhs.states_exp_only + rhs.states_rec_only, 1)<1e-1))[0]\n",
    "H0[all_sus] = 0.99 * comp_dist\n",
    "H0[one_inf] = 0.01 * comp_dist\n",
    "S0 = H0.T.dot(household_population.states[:, ::4])\n",
    "E0 = H0.T.dot(household_population.states[:, 1::4])\n",
    "I0 = H0.T.dot(household_population.states[:, 2::4])\n",
    "R0 = H0.T.dot(household_population.states[:, 3::4])\n",
    "start_state = (1/model_input.ave_hh_size) * array([S0.sum(),\n",
    "                                                   E0.sum(),\n",
    "                                                   I0.sum(),\n",
    "                                                   R0.sum()])\n",
    "tspan = (0.0, 365)\n",
    "print(\"(S,E,I,R)(0)=\",start_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0833e-a254-4715-be6a-b0fe88f4c5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now set up evaluation time points and solve system:\n",
    "\n",
    "# New time at which we evaluate the infection\n",
    "trange = np.arange(0,7*12,7) # Evaluate for 12 weeks\n",
    "\n",
    "# Solve:\n",
    "solution = solve_ivp(rhs, tspan, H0, first_step=0.001, atol=1e-16, t_eval=trange)\n",
    "\n",
    "T = solution.t\n",
    "H = solution.y\n",
    "S = H.T.dot(household_population.states[:, ::4])\n",
    "E = H.T.dot(household_population.states[:, 1::4])\n",
    "I = H.T.dot(household_population.states[:, 2::4])\n",
    "R = H.T.dot(household_population.states[:, 3::4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bc0f6-1125-40b3-b9a7-6717bb6d4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [S/model_input.ave_hh_by_class,\n",
    "    E/model_input.ave_hh_by_class,\n",
    "    I/model_input.ave_hh_by_class,\n",
    "    R/model_input.ave_hh_by_class]\n",
    "\n",
    "lgd=['S','E','I','R']\n",
    "\n",
    "fig, (axis) = subplots(1,1, sharex=True)\n",
    "\n",
    "cmap = get_cmap('tab20')\n",
    "alpha = 0.5\n",
    "for i in range(len(data_list)):\n",
    "    axis.plot(\n",
    "        T, data_list[i], label=lgd[i],\n",
    "        color=cmap(i/len(data_list)), alpha=alpha)\n",
    "axis.set_ylabel('Proportion of population')\n",
    "axis.legend(ncol=1, bbox_to_anchor=(1,0.50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54797f43-ea03-4efe-ae2b-2ff2627d9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "H[H<0.0] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b7b34-54cb-4db6-8e0a-a4a7b647432c",
   "metadata": {},
   "source": [
    "Make test data for single-household trajectories and calculate LLH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7ad65-34d7-40be-b69e-68bec3d159fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "H0 = make_initial_condition_by_eigenvector(growth_rate, model_input, household_population, rhs, 1e-1, 0.0,False,3)\n",
    "test_times = np.arange(7,7*12,7)\n",
    "def generate_single_hh_test_data(test_times):\n",
    "    Ht = deepcopy(H0)\n",
    "    test_data = np.zeros((len(test_times),))\n",
    "    for i in range(len(test_times)-1):\n",
    "        tspan = (test_times[i], test_times[i+1])\n",
    "        solution = solve_ivp(rhs, tspan, Ht, first_step=0.001, atol=1e-16)\n",
    "        T = solution.t\n",
    "        H = solution.y\n",
    "        state = choice(range(len(H[:, -1])), 1, p=H[:, -1]/sum(H[:, -1]))\n",
    "        test_data[i] = rhs.states_inf_only[state]\n",
    "        Ht *= 0\n",
    "        Ht[state] = 1\n",
    "    return(test_data)\n",
    "sample_data = generate_single_hh_test_data(test_times)\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93133763-b812-414d-b6f0-472026ee8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log likelihood if we can only measure infecteds\n",
    "# This is for test results from one household, youÂ´ll need to adapt to multiple\n",
    "\n",
    "def llh_from_test_data(test_data, test_ts, rhs, H0):\n",
    "    Ht = deepcopy(H0)\n",
    "    llh = 0\n",
    "    for i in range(len(test_times)-1):\n",
    "        if i==0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = test_times[i-1]\n",
    "        tspan = (start_time, test_times[i])\n",
    "        solution = solve_ivp(rhs, tspan, Ht, first_step=0.001, atol=1e-16)\n",
    "        T = solution.t\n",
    "        H = solution.y\n",
    "        I = test_data[i]\n",
    "        possible_states = np.where(np.abs(np.sum(rhs.states_inf_only,1)-I)<1e-1)[0]\n",
    "        llh += np.log(np.sum(H[possible_states, -1]))\n",
    "        Ht *= 0\n",
    "        Ht[possible_states] = H[possible_states, -1]\n",
    "    return(llh)\n",
    "\n",
    "llh_from_test_data(sample_data, test_times, rhs, H0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0ae75-6347-4a84-a060-4d3f6fbfce50",
   "metadata": {},
   "source": [
    "Quick check to make sure the restricted trajectory locks on to the right thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e5b59-0916-455b-8ec9-91de08e1c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log likelihood calculation but return probability trajectory\n",
    "\n",
    "def llh_with_traj(test_data, test_times, rhs, H0):\n",
    "    Ht = deepcopy(H0)\n",
    "    H_all = np.atleast_2d(deepcopy(H0)).T\n",
    "    t_all = np.array(0)\n",
    "    llh = 0\n",
    "    for i in range(len(test_times)):\n",
    "        if i==0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = test_times[i-1]\n",
    "        tspan = (start_time, test_times[i])\n",
    "        solution = solve_ivp(rhs, tspan, Ht, first_step=0.001, atol=1e-16)\n",
    "        T = solution.t\n",
    "        H = solution.y\n",
    "        I = test_data[i]\n",
    "        possible_states = np.where(np.abs(np.sum(rhs.states_inf_only,1)-I)<1e-1)[0]\n",
    "        llh += np.log(np.sum(H[possible_states, -1]))\n",
    "        Ht *= 0\n",
    "        Ht[possible_states] = H[possible_states, -1]\n",
    "        Ht = Ht/sum(Ht)\n",
    "        H_all = np.hstack((H_all, H))\n",
    "        t_all = np.hstack((t_all, T))\n",
    "    return(llh, H_all, t_all)\n",
    "\n",
    "llh, H_all, t_all = llh_with_traj(sample_data, test_times, rhs, H0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e6a109-faf7-4c27-abe7-a70594a44120",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = H_all.T.dot(household_population.states[:, ::4])\n",
    "E = H_all.T.dot(household_population.states[:, 1::4])\n",
    "I = H_all.T.dot(household_population.states[:, 2::4])\n",
    "R = H_all.T.dot(household_population.states[:, 3::4])\n",
    "\n",
    "data_list = [S/model_input.ave_hh_by_class,\n",
    "    E/model_input.ave_hh_by_class,\n",
    "    I/model_input.ave_hh_by_class,\n",
    "    R/model_input.ave_hh_by_class]\n",
    "\n",
    "lgd=['S','E','I','R', \"Test data\"]\n",
    "\n",
    "fig, (axis) = subplots(1,1, sharex=True)\n",
    "\n",
    "cmap = get_cmap('tab20')\n",
    "alpha = 0.5\n",
    "for i in range(len(data_list)):\n",
    "    axis.plot(t_all,\n",
    "        data_list[i], label=lgd[i],\n",
    "        color=cmap(i/len(data_list)), alpha=alpha)\n",
    "axis.plot(test_times, sample_data/model_input.ave_hh_by_class, marker=\".\", ls=\"\", ms=20, label=lgd[-1])\n",
    "axis.set_ylabel('Proportion of population')\n",
    "axis.legend(ncol=1, bbox_to_anchor=(1,0.50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8619ac-5f73-458d-8c34-2831bff08f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do multiple households\n",
    "\n",
    "# Generate test data:\n",
    "n_hh = 100\n",
    "multi_hh_data = [generate_single_hh_test_data(test_times) for i in range(n_hh)]\n",
    "\n",
    "# Option: Only include houses with at least one +ve\n",
    "# multi_hh_data = [data for data in multi_hh_data if np.sum(data)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fc999-5fd1-42ae-8675-0f472a722cdd",
   "metadata": {},
   "source": [
    "Have a look at the household data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bfce6a-fe9d-4fac-a5f1-9ede94319fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_hh_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507ac0e-aa1b-4056-bf42-a460025a11bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save testing data - it's big and slow to calculate so we don't want to have to redo this\n",
    "\n",
    "with open('outputs/inference-from-testing/synthetic-testing-data.pkl', 'wb') as f:\n",
    "    dump((multi_hh_data), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ce4cf-6d78-45d6-a5f4-95f823339638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a single likelihood calculation and see how long it takes to evaluate\n",
    "\n",
    "start_time = time.time()\n",
    "multi_hh_llh = sum(array([llh_from_test_data(sample_data, test_times, rhs, H0) for sample_data in multi_hh_data]))\n",
    "print(\"Single calculation takes\", time.time() - start_time,\"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29282739-869a-477f-a37e-d3f1bb18eaab",
   "metadata": {},
   "source": [
    "At this point, we should be able to write a function taking parameters as input and calculating llh for those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f86193-a11f-4cc9-aaab-2eb9c66ac045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llh_from_pars(data, test_times, tau, lam):\n",
    "    model_input = SEIRInput(SPEC, composition_list, comp_dist, print_ests=False)\n",
    "    model_input.k_home = (tau / model_input.beta_int) * model_input.k_home\n",
    "    model_input.k_ext = 0 * model_input.k_ext\n",
    "    model_input.density_expo = true_density_expo\n",
    "    # With the parameters chosen, we calculate Q_int:\n",
    "    household_population = HouseholdPopulation(\n",
    "        composition_list, comp_dist, model_input)\n",
    "    # lam goes in as arg for rate equations\n",
    "    rhs = SEIRRateEquations(model_input, household_population, FixedImportModel(4,1, np.array([lam * pop_prev])))\n",
    "    H0 = make_initial_condition_by_eigenvector(growth_rate, model_input, household_population, rhs, 1e-2, 0.0,False,3)\n",
    "    tspan = (0.0, 365)\n",
    "    return(sum(array([llh_from_test_data(data, test_times, rhs, H0) for data in multi_hh_data])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a863ef-2ad6-4b16-a8eb-715668c797cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llh_from_pars(multi_hh_data, test_times, 0.015, 2.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f1e55-02b2-48e7-a8d1-1010b02b7548",
   "metadata": {},
   "source": [
    "Now try calculating over a range of values and see what the likelihood curve looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be7379-8df1-41df-877e-1a2ffdbbe199",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_vals = arange(0.060, 0.105, 0.01)\n",
    "lam_vals = arange(2., 4.0, 0.25)\n",
    "\n",
    "llh_fixed_lam = np.zeros((len(tau_vals),))\n",
    "for i in range(len(tau_vals)):\n",
    "    print(\"tau=\",tau_vals[i])\n",
    "    llh_fixed_lam[i] = llh_from_pars(multi_hh_data, test_times, tau_vals[i], true_lam)\n",
    "\n",
    "llh_fixed_tau = np.zeros((len(lam_vals),))\n",
    "for i in range(len(lam_vals)):\n",
    "    print(\"lam=\",lam_vals[i])\n",
    "    llh_fixed_tau[i] = llh_from_pars(multi_hh_data, test_times, model_input.beta_int, lam_vals[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971342f-c7f6-494c-b80f-1f95353be246",
   "metadata": {},
   "source": [
    "Try using a root finder approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db961020-87a5-4cd6-be7a-f1066c6dbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tau_lam_mles(data,test_times,tau_0,lam_0):\n",
    "    def f(params):\n",
    "        tau = params[0]\n",
    "        lam = params[1]\n",
    "        return -llh_from_pars(data, test_times, tau, lam)\n",
    "\n",
    "    mle=sp.optimize.minimize(f,[tau_0,lam_0],bounds=((0.005, 0.15),(2., 5.)))\n",
    "    return mle.x[0],mle.x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1afa5d-ab8b-4343-b508-279fc912939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tau_hat, lam_hat = get_tau_lam_mles(multi_hh_data, test_times, 0.02, 2.5)\n",
    "end_time = time.time()\n",
    "print(\"Parameter estimation takes\", (end_time - start_time)/60,\"minutes.\")\n",
    "print(\"MLE of tau=\", tau_hat)\n",
    "print(\"MLE of lam=\", lam_hat)\n",
    "\n",
    "with open('outputs/inference-from-testing/synth-data-par-ests.pkl', 'wb') as f:\n",
    "    dump((tau_vals, lam_vals, llh_fixed_lam, llh_fixed_tau, tau_hat, lam_hat), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf83b1c-f184-412e-a429-5b321e506372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots, sharing the y-axis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "# Plot log-likelihood vs tau on the first subplot\n",
    "ax1.plot(tau_vals, llh_fixed_lam, label='Log likelihood vs tau')\n",
    "ax1.set_xlabel(\"tau\")\n",
    "ax1.set_ylabel(\"log likelihood\")\n",
    "\n",
    "# Plot the true value for lambda\n",
    "ax1.plot(np.array([model_input.beta_int, model_input.beta_int]), np.array([llh_fixed_lam.min(), llh_fixed_lam.max()]), \"k--\", label='True value for lambda')\n",
    "\n",
    "# Plot the estimated value for lambda\n",
    "ax1.plot(np.array([tau_hat, tau_hat]), np.array([llh_fixed_lam.min(), llh_fixed_lam.max()]), \"r--\", label='Estimated value for lambda')\n",
    "\n",
    "# Add a legend to the first subplot\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# Plot log-likelihood vs lambda on the second subplot\n",
    "ax2.plot(lam_vals, llh_fixed_tau, label='Log likelihood vs lambda')\n",
    "\n",
    "# Set the x-axis label for the second subplot\n",
    "ax2.set_xlabel(\"lambda\")\n",
    "\n",
    "# Plot the true value for tau\n",
    "ax2.plot(np.array([3., 3.]), np.array([llh_fixed_tau.min(), llh_fixed_tau.max()]), \"k--\", label='True value for tau')\n",
    "\n",
    "# Plot the estimated value for tau\n",
    "ax2.plot(np.array([lam_hat, lam_hat]), np.array([llh_fixed_tau.min(), llh_fixed_tau.max()]), \"r--\", label='Estimated value for tau')\n",
    "\n",
    "# Add a legend to the second subplot\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('n=20')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad7d84-1e9c-416c-ae3b-296070a8a2ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = subplots(1, 2, sharey=True)\n",
    "ax1.plot(tau_vals, llh_fixed_lam, label='Log likelihood vs tau')\n",
    "ax1.set_xlabel(\"tau\")\n",
    "ax1.set_ylabel(\"log likelihood\")\n",
    "ax1.plot(array(([model_input.beta_int, model_input.beta_int])), array(([llh_fixed_lam.min(), llh_fixed_lam.max()])), \"k--\", label='true value for lambda')\n",
    "ax1.plot(array(([tau_hat, tau_hat])), array(([llh_fixed_lam.min(), llh_fixed_lam.max()])), \"r--\", label='estimated value for lambda')\n",
    "ax1.legend(loc='lower right')\n",
    "ax2.plot(lam_vals, llh_fixed_tau, label='Log likelihood vs lambda')\n",
    "ax2.set_xlabel(\"lambda\")\n",
    "ax2.set_ylabel(\"log likelihood\")\n",
    "ax2.plot(array(([3., 3.])), array(([llh_fixed_tau.min(), llh_fixed_tau.max()])), \"k--\", label='true value for tau')\n",
    "ax2.plot(array(([lam_hat, lam_hat])), array(([llh_fixed_tau.min(), llh_fixed_tau.max()])), \"r--\", label='estimated value for tau')\n",
    "ax2.legend(loc='lower right')\n",
    "plt.savefig('n=20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d8475-32fd-489d-992a-4b17c041b210",
   "metadata": {},
   "source": [
    "Next two cells calculate likelihood over 2D parameter array. I don't suggest running this unless you're really interested in seeing the likelihood surfaces as it's inherently very slow. They are commented out for safety!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bc922-de94-4359-85d8-d7d12a97b748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tau_vals = arange(0.060, 0.105, 0.01)\n",
    "# lam_vals = arange(2., 4.0, 0.25)\n",
    "# llh_vals = np.zeros((len(tau_vals), len(lam_vals)))\n",
    "# for i in range(len(tau_vals)):\n",
    "#     for j in range(len(lam_vals)):\n",
    "#         llh_vals[i,j] = llh_from_pars(multi_hh_data, test_times, tau_vals[i], lam_vals[j])\n",
    "#         print(\"tau=\",tau_vals[i],\", lam=\",lam_vals[j], \", llh[tau, lam]=\", llh_vals[i, j])\n",
    "#with open('outputs/inference-from-testing/synth-data-gridded-par-ests.pkl', 'wb') as f:\n",
    "    # dump((tau_vals, lam_vals, llh_vals), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933457d-951f-4925-8aa1-3649cd611933",
   "metadata": {},
   "outputs": [],
   "source": [
    " # fig, ax = subplots(1, 1)\n",
    " # lam_inc = lam_vals[1] - lam_vals[0]\n",
    " # lam_max = lam_vals[-1] + (lam_vals[1]-lam_vals[0])\n",
    " # lam_range = lam_max - lam_vals[0]\n",
    " # tau_inc = tau_vals[1] - tau_vals[0]\n",
    " # tau_max = tau_vals[-1] + (tau_vals[1]-tau_vals[0])\n",
    " # tau_range = tau_max - tau_vals[0]\n",
    " # ax.imshow(llh_vals,\n",
    " #             origin='lower',\n",
    " #             extent=(lam_vals[0]-0.5*lam_inc,lam_max-0.5*lam_inc,tau_vals[0]-0.5*tau_inc,tau_max-0.5*tau_inc),\n",
    " #          aspect=lam_range/tau_range)\n",
    " # ax.set_xlabel(\"tau\")\n",
    " # ax.set_ylabel(\"lambda\")\n",
    " # ax.plot([true_lam],\n",
    " #         [model_input.beta_int],\n",
    " #         marker=\".\",\n",
    " #         ms=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab374597-79f5-4e35-b215-5d579259e55f",
   "metadata": {},
   "source": [
    "New idea: do fits for lots of small households, and see what the distribution of MLE's looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceda0a0-3582-47bd-8099-fb4798116b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLh for Hessian\n",
    "def neg_llh(x):\n",
    "    tau = x[0]\n",
    "    lam = x[1]\n",
    "    return -llh_from_pars(multi_hh_data, test_times, tau, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f455ab-4508-4201-9627-4aeec4416eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the hessian\n",
    "\n",
    "import numdifftools as nd\n",
    "#llh_from_pars has to be negtive, this was the llh not the -llh\n",
    "hess=nd.Hessian(neg_llh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc91d3-23f6-4995-b35c-1eb20c903935",
   "metadata": {},
   "outputs": [],
   "source": [
    "hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3ab76-0686-48ba-a67f-261ac8c37304",
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = (tau_hat, lam_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ecd06b-4864-4c18-b0d2-9b717ae60a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cholesky decomposition inversion\n",
    "cdi =  np.linalg.inv(np.linalg.cholesky(hess(xhat)))\n",
    "inverse = np.dot(cdi.T, cdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544479d1-7093-47ee-8c39-a4bd9599d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standart deviation\n",
    "stds = np.sqrt(np.diag(inverse))\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a9846-d43a-4f71-9fe7-1c298c72b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat + 1.96*stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68337ac1-36ba-4ecc-99b8-df12768253be",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_samples = 10\n",
    "n_hh = 5\n",
    "tau_mle_list = []\n",
    "lam_mle_list = []\n",
    "for smpl_no in range(no_samples):\n",
    "    print(\"i=\", smpl_no)\n",
    "    multi_hh_data = [generate_single_hh_test_data(test_times) for i in range(n_hh)]\n",
    "    tau_hat, lam_hat = get_tau_lam_mles(multi_hh_data, test_times, 0.02, 2.5)\n",
    "    tau_mle_list.append(tau_hat)\n",
    "    lam_mle_list.append(lam_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2053b9e2-9bc5-4cef-8af0-2e12d3c8a203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
